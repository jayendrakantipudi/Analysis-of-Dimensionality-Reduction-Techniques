{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71xaGNUVVtt6"
      },
      "source": [
        "### Import Libraries\n",
        "Loading all libraries to be used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX9T7PyeVtt8"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.decomposition import KernelPCA \n",
        "from time import time\n",
        "from numpy.linalg import eigh\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZf5ZMg4Vtt9"
      },
      "source": [
        "### Data preparation\n",
        "#### Load data\n",
        "Lets load the data from _dsjVoxArticles.tsv_ file. We will clean the title to remove special characters and punctuations. We will store title in _titles_ and Category in _categories_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvUbQ4YeVtt9"
      },
      "outputs": [],
      "source": [
        "df = fetch_20newsgroups(subset='all', shuffle=False, remove=('headers', 'quotes'))\n",
        "print(\"Message-\\n\", df.data[0])\n",
        "print(\"\\Label-\\n\", df.target_names[df.target[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZxJXklHVtt9"
      },
      "source": [
        "We can print and check the data loaded in _titles_ and _categories_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZdZxzMzUVtt9"
      },
      "outputs": [],
      "source": [
        "titles, categories = df.data, df.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDVTTfCTVtt9"
      },
      "source": [
        "### Split data\n",
        "Split data into 3 parts - training, development and test. We will use training data to train out model and use development data to check and tune hyper parameters. And finally use test data to see how our model performs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8v4dDFZVtt-"
      },
      "outputs": [],
      "source": [
        "title_tr, title_te, category_tr, category_te = train_test_split(titles, categories)\n",
        "print(\"Training: \",len(title_tr))\n",
        "print(\"Testing: \",len(title_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZlUUuVPVtt-"
      },
      "source": [
        "Using wordCload we can visualize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb93-xYzVtt-"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEasGvNaccN3"
      },
      "outputs": [],
      "source": [
        "df.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ik46_4DVtt-"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "sci_med, politics = [], []\n",
        "for (msg, lab) in zip(title_tr, category_tr):\n",
        "  if lab == 13:\n",
        "    sci_med.append(msg)\n",
        "  if lab == 16:\n",
        "    politics.append(msg)\n",
        "\n",
        "print(\"sci.med class word cloud\\n\")\n",
        "text = \" \".join(sci_med)\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.figure()\n",
        "plt.subplots(figsize=(20,12))\n",
        "wordcloud = WordCloud(\n",
        "    background_color=\"white\",\n",
        "    max_words=len(text),\n",
        "    max_font_size=40,\n",
        "    relative_scaling=.5).generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\\n\\n\\ntalk.politics.guns class word cloud\\n\")\n",
        "text = \" \".join(politics)\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.figure()\n",
        "plt.subplots(figsize=(20,12))\n",
        "wordcloud = WordCloud(\n",
        "    background_color=\"white\",\n",
        "    max_words=len(text),\n",
        "    max_font_size=40,\n",
        "    relative_scaling=.5).generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzeU-d_UVtt_"
      },
      "source": [
        "### Data Preprocessing\n",
        "#### Word lemmatization\n",
        "Lemmatize the messages using WordNet Lemmetizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nifkXEoQVtt_"
      },
      "outputs": [],
      "source": [
        "word_lemmatizer = WordNetLemmatizer()\n",
        "stopwords_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "for i in range(len(title_tr)):\n",
        "  msg_words = word_tokenize(title_tr[i])\n",
        "  lemmatized_msg = \"\"\n",
        "  for word in msg_words:\n",
        "    if word not in stopwords_set:\n",
        "      lemmatized_msg += word_lemmatizer.lemmatize(word) + \" \"\n",
        "  title_tr[i] = lemmatized_msg\n",
        "\n",
        "for i in range(len(title_te)):\n",
        "  msg_words = word_tokenize(title_te[i])\n",
        "  lemmatized_msg = \"\"\n",
        "  for word in msg_words:\n",
        "    if word not in stopwords_set:\n",
        "      lemmatized_msg += word_lemmatizer.lemmatize(word) + \" \"\n",
        "  title_te[i] = lemmatized_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGn7Ea-cAa74"
      },
      "source": [
        "### Vectorization of data\n",
        "Vectorize the data using Bag of words (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1zYZhkjkoV_q"
      },
      "outputs": [],
      "source": [
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=0.001)\n",
        "vectorizer.fit(title_tr)\n",
        "\n",
        "Xtr = vectorizer.transform(title_tr)\n",
        "Xte = vectorizer.transform(title_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT-5uu4BVtt_"
      },
      "source": [
        "Lets look at what exactly is this vectorizer doing. We will first create reverse dictionary from the vectorizer. Iterating over the vectorized sentence _Nasa scientists are good_. We get the vector to be representative of three words \"good\", \"nasa\" and \"scientists\". The order has been changed because bag of words does not preserve order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1ksZVcPVtt_",
        "outputId": "1323b9ed-e2d4-489a-86ca-5c63615eac80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top season rotation morgan lead helped far even era cubs better atlanta "
          ]
        }
      ],
      "source": [
        "reverse_vocabulary = {}\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "for word in vocabulary:\n",
        "    index = vocabulary[word]\n",
        "    reverse_vocabulary[index] = word\n",
        "\n",
        "vector = vectorizer.transform(iter([\"This season so far, Morgan and Guzman helped to lead the Cubs at top in ERA, even better than THE rotation at Atlanta.\"]))\n",
        "indexes = vector.indices\n",
        "for i in indexes:\n",
        "    print (reverse_vocabulary[i], end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HArNqVVTVtt_"
      },
      "source": [
        "### Frequency Threshold\n",
        "We can check the variance of the feature and drop them based on a threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUUsGkqeVtt_",
        "outputId": "77707ca1-7b3a-40dc-de5e-ceefdd940590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features before reduction :  9307\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of features before reduction : \", Xtr.shape[1])\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "scaled_data_Xtr = scaler.fit_transform(Xtr)\n",
        "scaled_data_Xte = scaler.transform(Xte)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "def purity_score(y, yhat):\n",
        "  c_mat = metrics.cluster.contingency_matrix(y, yhat)\n",
        "  sns.set(rc={'figure.figsize':(20,20)})\n",
        "  print(sns.heatmap(c_mat, annot=True, ))\n",
        "  return np.sum(np.amax(c_mat, axis=0)) / np.sum(c_mat)"
      ],
      "metadata": {
        "id": "FRrUVhiQKRB2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without Reduction\n",
        "Results calculated with any features reduction on the dataset"
      ],
      "metadata": {
        "id": "OA-0D2BTL-3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Random Forest"
      ],
      "metadata": {
        "id": "B3oaQdsRMH4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "start_time = time()\n",
        "rf.fit(scaled_data_Xtr.toarray(), category_tr)\n",
        "print(\"Time taken to fit Random Forest classifier : \", time()-start_time)\n",
        "pred = rf.predict(scaled_data_Xte.toarray())\n",
        "print(classification_report(category_te, pred, target_names=df.target_names, digits=5))"
      ],
      "metadata": {
        "id": "h_1E35KqMLo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### SVC"
      ],
      "metadata": {
        "id": "Mp3MKkjwMNnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC()\n",
        "start_time = time() \n",
        "svc.fit(scaled_data_Xtr.toarray(), category_tr)\n",
        "print(\"Time taken to fit SVC classifier : \", time()-start_time)\n",
        "pred = svc.predict(scaled_data_Xte.toarray())\n",
        "print(classification_report(category_te, pred, target_names=df.target_names, digits=5))"
      ],
      "metadata": {
        "id": "w9A6RFJ8MPQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### KMeans"
      ],
      "metadata": {
        "id": "FW_7or6DMP-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=20, init='k-means++', max_iter=250)\n",
        "start_time = time()\n",
        "kmeans.fit(scaled_data_Xtr.toarray(), category_tr)\n",
        "print(\"Time taken to fit KMeans classifier : \", time()-start_time)\n",
        "pred = kmeans.predict(scaled_data_Xte.toarray())\n",
        "print(\"Purity Score: \", purity_score(category_te, pred))\n",
        "print(\"Homoegneity Score: \",metrics.homogeneity_score(category_te, pred))\n",
        "print(\"Completeness Score: \",metrics.completeness_score(category_te, pred))"
      ],
      "metadata": {
        "id": "_7MXDoyGMRzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHN3wp1rlmtt"
      },
      "source": [
        "### PCA (Principal Component Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNQYFu48loVy"
      },
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(scaled_data_Xtr.toarray())\n",
        "reduced_pca_Xtr = pca.transform(scaled_data_Xtr.toarray())\n",
        "reduced_pca_Xte = pca.transform(scaled_data_Xte.toarray())\n",
        "print(\"Time taken to perform dimensionality reduction using PCA\" ,time() - start_time)\n",
        "print(\"Number of features after reduction : \", reduced_pca_Xtr.shape[1])\n",
        "print(\"Explained variance ratio: \", sum(pca.explained_variance_ratio_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WhzSlB5w43q"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.style.use(\"ggplot\") \n",
        "plt.plot(pca.explained_variance_, marker='.')\n",
        "plt.xlabel(\"Eigenvalue number\")\n",
        "plt.ylabel(\"Eigenvalue size\")\n",
        "plt.title(\"Scree Plot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Random Forest"
      ],
      "metadata": {
        "id": "bFMBNWI9IqbQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E6QG6ajG7nQ"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier()\n",
        "start_time = time()\n",
        "rf.fit(reduced_pca_Xtr, category_tr)\n",
        "print(\"Time taken to fit Random Forest classifier : \", time()-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = rf.predict(reduced_pca_Xte)\n",
        "print(classification_report(category_te, pred, target_names=df.target_names, digits=5))"
      ],
      "metadata": {
        "id": "DPUPdqQaDuHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### SVC"
      ],
      "metadata": {
        "id": "NfK3qZVvJTaz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hLKN5XuO3pX"
      },
      "outputs": [],
      "source": [
        "svc = SVC()\n",
        "start_time = time() \n",
        "svc.fit(reduced_pca_Xtr, category_tr)\n",
        "print(\"Time taken to fit SVC classifier : \", time()-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = svc.predict(reduced_pca_Xte)\n",
        "print(classification_report(category_te, pred, target_names=df.target_names))"
      ],
      "metadata": {
        "id": "BvVCWTMuJZum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### KMeans"
      ],
      "metadata": {
        "id": "F08OWdncJX8s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL8YymkQb6c4"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=20, init='k-means++', max_iter=250)\n",
        "start_time = time()\n",
        "kmeans.fit(reduced_pca_Xtr, category_tr)\n",
        "print(\"Time taken to fit KMeans classifier : \", time()-start_time)\n",
        "pred = kmeans.predict(reduced_pca_Xte)\n",
        "print(\"Purity Score: \", purity_score(category_te, pred))\n",
        "print(\"Homoegneity Score: \",metrics.homogeneity_score(category_te, pred))\n",
        "print(\"Completeness Score: \",metrics.completeness_score(category_te, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZwLSh1c6sFF"
      },
      "source": [
        "### LDA (Linear Discriminant Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "csSKNo1M6zzK"
      },
      "outputs": [],
      "source": [
        "lda = LDA(n_components=15)\n",
        "lda.fit(scaled_data_Xtr.toarray(), category_tr)\n",
        "reduced_lda_Xtr = lda.transform(scaled_data_Xtr.toarray())\n",
        "reduced_lda_Xte = lda.transform(scaled_data_Xte.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Random Forest"
      ],
      "metadata": {
        "id": "tbw2i74VKZEa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub-v5tJv7Ci-"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier()\n",
        "start_time = time()\n",
        "rf.fit(reduced_lda_Xtr, category_tr)\n",
        "print(\"Time taken for fitting using Random Forest\",time() - start_time)\n",
        "pred = rf.predict(reduced_lda_Xte)\n",
        "print(classification_report(category_te, pred, target_names=df.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### SVC"
      ],
      "metadata": {
        "id": "GG06nBQvKdJw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rXl2P8p7IeI"
      },
      "outputs": [],
      "source": [
        "svc = SVC()\n",
        "start_time = time()\n",
        "svc.fit(reduced_lda_Xtr, category_tr)\n",
        "print(\"Time taken for fitting using SVC\",time() - start_time)\n",
        "pred = svc.predict(reduced_lda_Xte)\n",
        "print(classification_report(category_te, pred, target_names=df.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### KMeans"
      ],
      "metadata": {
        "id": "Gu4-R4wAKgXo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvkK3LCs7LFG"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=20, init='k-means++', max_iter=250)\n",
        "start_time = time()\n",
        "kmeans.fit(reduced_lda_Xtr, category_tr)\n",
        "print(\"Time taken for fitting using KMeans\",time() - start_time)\n",
        "pred = kmeans.predict(reduced_lda_Xte)\n",
        "print(\"Purity Score: \", purity_score(category_te, pred))\n",
        "print(\"Homoegneity Score: \",metrics.homogeneity_score(category_te, pred))\n",
        "print(\"Completeness Score: \",metrics.completeness_score(category_te, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW7gTe-I7OWk"
      },
      "source": [
        "### Kernel PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQGAxlne7TSt"
      },
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "kpca = KernelPCA(kernel=\"sigmoid\", n_components=2, gamma=.01) # Sigmoid Kernel\n",
        "reduced_kpca_Xtr = kpca.fit_transform(scaled_data_Xtr)\n",
        "reduced_kpca_Xte = kpca.transform(scaled_data_Xte.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_zc0RyrhU4R"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XIetyJ4hTDV"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier()\n",
        "start_time = time()\n",
        "rf.fit(reduced_kpca_Xtr, category_tr)\n",
        "print(\"Time taken to fit Random Forest classifier : \", time()-start_time)\n",
        "pred = rf.predict(reduced_kpca_Xte)\n",
        "print(classification_report(category_te, pred, target_names=df.target_names,digits=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC"
      ],
      "metadata": {
        "id": "y34zagLqQuL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC()\n",
        "start_time = time()\n",
        "svc.fit(reduced_kpca_Xtr, category_tr)\n",
        "print(\"Time taken for fitting using SVC\",time() - start_time)\n",
        "pred = svc.predict(reduced_kpca_Xte)\n",
        "print(classification_report(category_te, pred, target_names=df.target_names))"
      ],
      "metadata": {
        "id": "ldqlQbvSQtYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans"
      ],
      "metadata": {
        "id": "z6-kiqbjQvii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=20, init='k-means++', max_iter=250)\n",
        "start_time = time()\n",
        "kmeans.fit(reduced_kpca_Xtr, category_tr)\n",
        "print(\"Time taken for fitting using KMeans\",time() - start_time)\n",
        "pred = kmeans.predict(reduced_kpca_Xte)\n",
        "print(\"Purity Score: \", purity_score(category_te, pred))\n",
        "print(\"Homoegneity Score: \",metrics.homogeneity_score(category_te, pred))\n",
        "print(\"Completeness Score: \",metrics.completeness_score(category_te, pred))"
      ],
      "metadata": {
        "id": "lhRBp4OhQw7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB5x4F3y7bfh"
      },
      "source": [
        "### UMAP (Unified Manifol Approximation & Projection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu-wXFMh7eL6"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6YFU_BG7fYP"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "umap_embedding = umap.UMAP(metric='hellinger', n_components=2)\n",
        "reduced_tr = umap_embedding.fit_transform(scaled_data_Xtr)\n",
        "reduced_te = umap_embedding.transform(scaled_data_Xte)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refined_tr = reduced_tr[[not a for a in umap.utils.disconnected_vertices(umap_embedding)]]\n",
        "refined_cat = category_tr[[not a for a in umap.utils.disconnected_vertices(umap_embedding)]]\n",
        "\n",
        "reduced_te = umap_embedding.transform(scaled_data_Xte)\n",
        "refined_te = []\n",
        "refined_cat_te = []\n",
        "for i in range(len(reduced_te)):\n",
        "  if str(reduced_te[i][0]) != \"nan\":\n",
        "    refined_te.append(reduced_te[i])\n",
        "    refined_cat_te.append(category_te[i])"
      ],
      "metadata": {
        "id": "woPCuwdnSW6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data_Xtr\n",
        "scaled_data_Xte"
      ],
      "metadata": {
        "id": "slUVRSopR3Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "yhtPJKk-SNH_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zzVgUZX3amZ"
      },
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(refined_tr, refined_cat)\n",
        "pred = rf.predict(refined_te)\n",
        "print(\"Time taken to fit Random Forest classifier : \", time()-start_time)\n",
        "print(classification_report(refined_cat_te, pred, target_names=df.target_names, digits=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC"
      ],
      "metadata": {
        "id": "jTCT-xORSOlu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3C0PMuR-kk3"
      },
      "outputs": [],
      "source": [
        "svc = SVC()\n",
        "start_time = time()\n",
        "svc.fit(refined_tr, refined_cat)\n",
        "print(\"Time taken for fit SVC classifier\",time() - start_time)\n",
        "pred = svc.predict(refined_te)\n",
        "print(classification_report(category_te, pred, target_names=df.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans"
      ],
      "metadata": {
        "id": "LmxLyWijSQQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=20, init='k-means++', max_iter=250)\n",
        "start_time = time()\n",
        "kmeans.fit(refined_tr, refined_cat)\n",
        "print(\"Time taken for fitting using KMeans\",time() - start_time)\n",
        "pred = kmeans.predict(refined_te)\n",
        "print(\"Purity Score: \", purity_score(category_te, pred))\n",
        "print(\"Homoegneity Score: \",metrics.homogeneity_score(category_te, pred))\n",
        "print(\"Completeness Score: \",metrics.completeness_score(category_te, pred))"
      ],
      "metadata": {
        "id": "V5wpkVCBS1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduced dataset visualization (n_components = 2)"
      ],
      "metadata": {
        "id": "F9dChuEPRpwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "umapdf = pd.DataFrame()\n",
        "umapdf[\"y\"] = [df.target_names[lab] for lab in category_tr]\n",
        "umapdf[\"comp-1\"] = reduced_tr[:,0]\n",
        "umapdf[\"comp-2\"] = reduced_tr[:,1]\n",
        "sns.set(rc={'figure.figsize':(20,10)})\n",
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=umapdf.y.tolist(), data=umapdf).set(title=\"PCA projection with features = 2\")"
      ],
      "metadata": {
        "id": "oohZr9UBS-W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "umapdf = pd.DataFrame()\n",
        "umapdf[\"y\"] = [df.target_names[lab] for lab in category_tr]\n",
        "umapdf[\"comp-1\"] = reduced_tr[:,0]\n",
        "umapdf[\"comp-2\"] = reduced_tr[:,1]\n",
        "sns.set(rc={'figure.figsize':(20,10)})\n",
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=umapdf.y.tolist(), data=umapdf).set(title=\"LDA projection with features = 2\")"
      ],
      "metadata": {
        "id": "QwxBWkLbS-Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "umapdf = pd.DataFrame()\n",
        "umapdf[\"y\"] = [df.target_names[lab] for lab in category_tr]\n",
        "umapdf[\"comp-1\"] = reduced_tr[:,0]\n",
        "umapdf[\"comp-2\"] = reduced_tr[:,1]\n",
        "sns.set(rc={'figure.figsize':(20,10)})\n",
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=umapdf.y.tolist(), data=umapdf).set(title=\"Kernel PCA projection with features = 2\")"
      ],
      "metadata": {
        "id": "sbYg6LQeS-K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pfwmVXr4Jqh"
      },
      "outputs": [],
      "source": [
        "umapdf = pd.DataFrame()\n",
        "umapdf[\"y\"] = [df.target_names[lab] for lab in category_tr]\n",
        "umapdf[\"comp-1\"] = reduced_tr[:,0]\n",
        "umapdf[\"comp-2\"] = reduced_tr[:,1]\n",
        "sns.set(rc={'figure.figsize':(20,10)})\n",
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=umapdf.y.tolist(), data=umapdf).set(title=\"UMAP projection with features = 2\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}